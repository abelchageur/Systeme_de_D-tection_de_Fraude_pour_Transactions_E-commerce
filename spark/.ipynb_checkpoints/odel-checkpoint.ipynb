{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a5b4d308-e0de-4c0e-9e70-7dd135ff6752",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/04/20 13:59:03 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session created successfully!\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "from pyspark.sql.functions import col, to_date, dayofmonth, dayofweek, month, when, lit, isnull, count\n",
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, GBTClassifier, DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from datetime import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Initialize Spark Session optimized for local Jupyter execution\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"FraudDetection\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.maxResultSize\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark session created successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b199b052-4afd-4dcb-8bab-fedd3bdd096f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully. Row count: 23634\n"
     ]
    }
   ],
   "source": [
    "# Define schema\n",
    "schema = StructType([\n",
    "    StructField(\"Transaction ID\", StringType(), True),\n",
    "    StructField(\"Customer ID\", StringType(), True),\n",
    "    StructField(\"Transaction Amount\", FloatType(), True),\n",
    "    StructField(\"Transaction Date\", StringType(), True),\n",
    "    StructField(\"Payment Method\", StringType(), True),\n",
    "    StructField(\"Product Category\", StringType(), True),\n",
    "    StructField(\"Quantity\", IntegerType(), True),\n",
    "    StructField(\"Customer Age\", IntegerType(), True),\n",
    "    StructField(\"Customer Location\", StringType(), True),\n",
    "    StructField(\"Device Used\", StringType(), True),\n",
    "    StructField(\"IP Address\", StringType(), True),\n",
    "    StructField(\"Shipping Address\", StringType(), True),\n",
    "    StructField(\"Billing Address\", StringType(), True),\n",
    "    StructField(\"Is Fraudulent\", IntegerType(), True),\n",
    "    StructField(\"Account Age Days\", IntegerType(), True),\n",
    "    StructField(\"Transaction Hour\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "# Load data with error handling\n",
    "try:\n",
    "    # For local Jupyter environment, use local file path - adjust this to your file location\n",
    "    # If you're using actual HDFS, keep the HDFS path\n",
    "    try:\n",
    "        # First try local file\n",
    "        file_path = \"transaction_data.csv\"  # Change this to your local file path\n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"multiLine\", \"true\") \\\n",
    "                       .schema(schema) \\\n",
    "                       .csv(file_path)\n",
    "    except:\n",
    "        # If local fails, try HDFS path\n",
    "        today = datetime.today()\n",
    "        hdfs_path = f\"hdfs://namenode:9000/user/root/transactions/YYYY={today.year}/MM={today.month:02d}/DD={today.day:02d}/transaction_data.csv\"\n",
    "        df = spark.read.option(\"header\", \"true\") \\\n",
    "                       .option(\"multiLine\", \"true\") \\\n",
    "                       .schema(schema) \\\n",
    "                       .csv(hdfs_path)\n",
    "    \n",
    "    print(\"Data loaded successfully. Row count:\", df.count())\n",
    "except Exception as e:\n",
    "    print(f\"Error loading data: {str(e)}\")\n",
    "    print(\"Please update the file path to point to your transaction data.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8fbfd777-9bbe-4e9e-b5f2-1623b13949be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Quality Check:\n",
      "Null values per column:\n",
      "-RECORD 0-----------------\n",
      " Transaction ID     | 0   \n",
      " Customer ID        | 0   \n",
      " Transaction Amount | 0   \n",
      " Transaction Date   | 0   \n",
      " Payment Method     | 0   \n",
      " Product Category   | 0   \n",
      " Quantity           | 0   \n",
      " Customer Age       | 0   \n",
      " Customer Location  | 0   \n",
      " Device Used        | 0   \n",
      " IP Address         | 0   \n",
      " Shipping Address   | 0   \n",
      " Billing Address    | 0   \n",
      " Is Fraudulent      | 0   \n",
      " Account Age Days   | 0   \n",
      " Transaction Hour   | 0   \n",
      "\n",
      "\n",
      "Sample data:\n",
      "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-------------------+-----------+---------------+--------------------+--------------------+-------------+----------------+----------------+\n",
      "|      Transaction ID|         Customer ID|Transaction Amount|   Transaction Date|Payment Method|Product Category|Quantity|Customer Age|  Customer Location|Device Used|     IP Address|    Shipping Address|     Billing Address|Is Fraudulent|Account Age Days|Transaction Hour|\n",
      "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-------------------+-----------+---------------+--------------------+--------------------+-------------+----------------+----------------+\n",
      "|c12e07a0-8a06-4c0...|8ca9f102-02a4-420...|             42.32|2024-03-24 23:42:43|        PayPal|     electronics|       1|          40|    East Jameshaven|    desktop|  110.87.246.85|5399 Rachel Strav...|5399 Rachel Strav...|            0|             282|              23|\n",
      "|7d187603-7961-4fc...|4d158416-caae-4b0...|            301.34|2024-01-22 00:53:31|   credit card|     electronics|       3|          35|           Kingstad|     tablet|  14.73.104.153|5230 Stephanie Fo...|5230 Stephanie Fo...|            0|             223|               0|\n",
      "|f2c14f9d-92df-4aa...|ccae47b8-75c7-4f5...|            340.32|2024-01-22 08:06:03|    debit card|    toys & games|       5|          29|         North Ryan|    desktop|    67.58.94.93|195 Cole Oval\\nPo...|4772 David Strave...|            0|             360|               8|\n",
      "|e9949bfa-194d-486...|b04960c0-aeee-490...|             95.77|2024-01-16 20:34:53|   credit card|     electronics|       5|          45|         Kaylaville|     mobile|202.122.126.216|7609 Cynthia Squa...|7609 Cynthia Squa...|            0|             325|              20|\n",
      "|7362837c-7538-434...|de9d6351-b3a7-4bc...|             77.45|2024-01-16 15:47:23|   credit card|        clothing|       5|          42|North Edwardborough|    desktop|   96.77.232.76|2494 Robert Ramp ...|2494 Robert Ramp ...|            0|             116|              15|\n",
      "+--------------------+--------------------+------------------+-------------------+--------------+----------------+--------+------------+-------------------+-----------+---------------+--------------------+--------------------+-------------+----------------+----------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "\n",
      "Fraudulent transactions: 1222 (5.17%)\n",
      "Non-fraudulent transactions: 22412 (94.83%)\n",
      "Imbalance ratio: 1:18.3\n"
     ]
    }
   ],
   "source": [
    "# Data Quality Check\n",
    "print(\"\\nData Quality Check:\")\n",
    "print(\"Null values per column:\")\n",
    "null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "null_counts.show(vertical=True)\n",
    "\n",
    "# Show sample data\n",
    "print(\"\\nSample data:\")\n",
    "df.show(5)\n",
    "\n",
    "# Check class distribution\n",
    "fraud_count = df.filter(col(\"Is Fraudulent\") == 1).count()\n",
    "non_fraud_count = df.filter(col(\"Is Fraudulent\") == 0).count()\n",
    "total = fraud_count + non_fraud_count\n",
    "fraud_ratio = fraud_count / total\n",
    "\n",
    "print(f\"\\nFraudulent transactions: {fraud_count} ({fraud_ratio:.2%})\")\n",
    "print(f\"Non-fraudulent transactions: {non_fraud_count} ({1-fraud_ratio:.2%})\")\n",
    "print(f\"Imbalance ratio: 1:{non_fraud_count/fraud_count:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed838f9a-7a18-4c95-b9c0-f37b01077058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Data Preprocessing...\n",
      "\n",
      "Preprocessing completed in -2.01 seconds\n",
      "Sample of weights (fraud cases should have higher weight):\n",
      "+-------------+------------------+\n",
      "|Is Fraudulent|       classWeight|\n",
      "+-------------+------------------+\n",
      "|            0|0.0517051705170517|\n",
      "|            1| 4.741474147414742|\n",
      "+-------------+------------------+\n",
      "\n",
      "\n",
      "All features after preprocessing:\n",
      "root\n",
      " |-- Transaction ID: string (nullable = true)\n",
      " |-- Customer ID: string (nullable = true)\n",
      " |-- Transaction Amount: float (nullable = true)\n",
      " |-- Transaction Date: date (nullable = true)\n",
      " |-- Payment Method: string (nullable = false)\n",
      " |-- Product Category: string (nullable = false)\n",
      " |-- Quantity: integer (nullable = true)\n",
      " |-- Customer Age: integer (nullable = true)\n",
      " |-- Customer Location: string (nullable = false)\n",
      " |-- Device Used: string (nullable = false)\n",
      " |-- IP Address: string (nullable = true)\n",
      " |-- Shipping Address: string (nullable = true)\n",
      " |-- Billing Address: string (nullable = true)\n",
      " |-- Is Fraudulent: integer (nullable = true)\n",
      " |-- Account Age Days: integer (nullable = true)\n",
      " |-- Transaction Hour: integer (nullable = true)\n",
      " |-- DayOfMonth: integer (nullable = true)\n",
      " |-- DayOfWeek: integer (nullable = true)\n",
      " |-- Month: integer (nullable = true)\n",
      " |-- AddressMismatch: integer (nullable = false)\n",
      " |-- AmountPerQuantity: double (nullable = true)\n",
      " |-- IsWeekend: integer (nullable = false)\n",
      " |-- IsNightTime: integer (nullable = false)\n",
      " |-- classWeight: double (nullable = false)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "print(\"\\nStarting Data Preprocessing...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert Transaction Date and extract features\n",
    "df = df.withColumn(\"Transaction Date\", to_date(col(\"Transaction Date\")))\n",
    "df = df.withColumn(\"DayOfMonth\", dayofmonth(col(\"Transaction Date\")))\n",
    "df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Transaction Date\")))\n",
    "df = df.withColumn(\"Month\", month(col(\"Transaction Date\")))\n",
    "\n",
    "# Create feature: is shipping different from billing\n",
    "df = df.withColumn(\"AddressMismatch\", \n",
    "                  when(col(\"Shipping Address\") != col(\"Billing Address\"), 1).otherwise(0))\n",
    "\n",
    "# Create feature: transaction amount per quantity\n",
    "df = df.withColumn(\"AmountPerQuantity\", \n",
    "                  when(col(\"Quantity\") > 0, col(\"Transaction Amount\") / col(\"Quantity\")).otherwise(col(\"Transaction Amount\")))\n",
    "\n",
    "# Create time-based features\n",
    "df = df.withColumn(\"IsWeekend\", \n",
    "                  when((col(\"DayOfWeek\") == 1) | (col(\"DayOfWeek\") == 7), 1).otherwise(0))\n",
    "\n",
    "df = df.withColumn(\"IsNightTime\", \n",
    "                  when((col(\"Transaction Hour\") >= 22) | (col(\"Transaction Hour\") <= 5), 1).otherwise(0))\n",
    "\n",
    "# Calculate class weights (to handle imbalance)\n",
    "# Higher weight for minority class (fraud)\n",
    "weight_multiplier = 5.0  # Increase this for better fraud detection\n",
    "fraud_weight = (non_fraud_count / total) * weight_multiplier\n",
    "non_fraud_weight = (fraud_count / total)\n",
    "\n",
    "df = df.withColumn(\"classWeight\", \n",
    "                  when(col(\"Is Fraudulent\") == 1, fraud_weight)\n",
    "                  .otherwise(non_fraud_weight))\n",
    "\n",
    "# Fill nulls in categorical columns\n",
    "categorical_cols = [\"Payment Method\", \"Product Category\", \"Customer Location\", \"Device Used\"]\n",
    "for col_name in categorical_cols:\n",
    "    df = df.fillna(\"unknown\", subset=[col_name])\n",
    "\n",
    "print(f\"\\nPreprocessing completed in {time.time() - start_time:.2f} seconds\")\n",
    "print(\"Sample of weights (fraud cases should have higher weight):\")\n",
    "df.select(\"Is Fraudulent\", \"classWeight\").distinct().orderBy(\"Is Fraudulent\").show(5)\n",
    "\n",
    "# Show all features\n",
    "print(\"\\nAll features after preprocessing:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b35b54c-3b69-4bbe-ad29-8c4019fc06b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature pipeline rebuilt with stages:\n",
      "1. StringIndexer\n",
      "2. OneHotEncoder\n",
      "3. StringIndexer\n",
      "4. OneHotEncoder\n",
      "5. StringIndexer\n",
      "6. OneHotEncoder\n",
      "7. StringIndexer\n",
      "8. OneHotEncoder\n",
      "9. Imputer\n",
      "10. VectorAssembler\n",
      "11. StandardScaler\n",
      "\n",
      "Train rows: 18916, Test rows: 4718\n",
      "\n",
      "Training Random Forest model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 14:03:07 WARN DAGScheduler: Broadcasting large task binary with size 2.3 MiB\n",
      "25/04/20 14:03:08 WARN DAGScheduler: Broadcasting large task binary with size 2.2 MiB\n",
      "25/04/20 14:03:09 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/04/20 14:03:10 WARN DAGScheduler: Broadcasting large task binary with size 2.5 MiB\n",
      "25/04/20 14:03:14 WARN DAGScheduler: Broadcasting large task binary with size 2.6 MiB\n",
      "25/04/20 14:03:58 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/20 14:04:14 WARN DAGScheduler: Broadcasting large task binary with size 2.8 MiB\n",
      "25/04/20 14:04:17 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/04/20 14:04:22 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/04/20 14:04:25 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/04/20 14:04:28 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/04/20 14:04:31 WARN DAGScheduler: Broadcasting large task binary with size 3.0 MiB\n",
      "25/04/20 14:04:34 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/04/20 14:04:38 WARN DAGScheduler: Broadcasting large task binary with size 3.1 MiB\n",
      "25/04/20 14:04:43 WARN DAGScheduler: Broadcasting large task binary with size 3.2 MiB\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training completed in 107.35 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/04/20 14:04:51 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "25/04/20 14:04:54 WARN DAGScheduler: Broadcasting large task binary with size 2.9 MiB\n",
      "[Stage 77:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.7829\n",
      "Recall: 0.0466\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# 1. Clear existing stages if any\n",
    "stages = []\n",
    "\n",
    "# 2. First, handle categorical columns\n",
    "categorical_cols = [\"Payment Method\", \"Product Category\", \"Customer Location\", \"Device Used\"]\n",
    "categorical_features = []\n",
    "\n",
    "# For each categorical column, create an indexed column and then one-hot encode it\n",
    "for col_name in categorical_cols:\n",
    "    # Create a string indexer\n",
    "    indexer = StringIndexer(\n",
    "        inputCol=col_name, \n",
    "        outputCol=f\"{col_name}_indexed\", \n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages.append(indexer)\n",
    "    \n",
    "    # Create a one-hot encoder\n",
    "    encoder = OneHotEncoder(\n",
    "        inputCol=f\"{col_name}_indexed\", \n",
    "        outputCol=f\"{col_name}_encoded\",\n",
    "        handleInvalid=\"keep\"\n",
    "    )\n",
    "    stages.append(encoder)\n",
    "    categorical_features.append(f\"{col_name}_encoded\")\n",
    "\n",
    "# 3. Handle numerical columns\n",
    "numerical_cols = [\"Transaction Amount\", \"Quantity\", \"Customer Age\", \n",
    "                  \"Account Age Days\", \"Transaction Hour\", \"DayOfMonth\",\n",
    "                  \"DayOfWeek\", \"Month\", \"AddressMismatch\", \"AmountPerQuantity\",\n",
    "                  \"IsWeekend\", \"IsNightTime\"]\n",
    "\n",
    "# Create an imputer for numerical columns\n",
    "imputer = Imputer(\n",
    "    inputCols=numerical_cols,\n",
    "    outputCols=[f\"{col}_imputed\" for col in numerical_cols],\n",
    "    strategy=\"median\"\n",
    ")\n",
    "stages.append(imputer)\n",
    "numerical_features = [f\"{col}_imputed\" for col in numerical_cols]\n",
    "\n",
    "# 4. Assemble all features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=categorical_features + numerical_features,\n",
    "    outputCol=\"raw_features\",\n",
    "    handleInvalid=\"keep\"\n",
    ")\n",
    "stages.append(assembler)\n",
    "\n",
    "# 5. Scale features\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"raw_features\", \n",
    "    outputCol=\"features\", \n",
    "    withStd=True, \n",
    "    withMean=True\n",
    ")\n",
    "stages.append(scaler)\n",
    "\n",
    "print(\"Feature pipeline rebuilt with stages:\")\n",
    "for i, stage in enumerate(stages):\n",
    "    print(f\"{i+1}. {type(stage).__name__}\")\n",
    "\n",
    "# Now let's try to train a simple model without hyperparameter tuning\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\", \n",
    "    labelCol=\"Is Fraudulent\", \n",
    "    weightCol=\"classWeight\",\n",
    "    numTrees=100,\n",
    "    maxDepth=10,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Split data (80% train, 20% test)\n",
    "train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "print(f\"\\nTrain rows: {train.count()}, Test rows: {test.count()}\")\n",
    "\n",
    "# Add the model to stages\n",
    "model_pipeline = Pipeline(stages=stages + [rf])\n",
    "\n",
    "# Train the model\n",
    "print(\"\\nTraining Random Forest model...\")\n",
    "start_time = time.time()\n",
    "try:\n",
    "    model = model_pipeline.fit(train)\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"Training completed in {training_time:.2f} seconds\")\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions = model.transform(test)\n",
    "    \n",
    "    # Evaluate model\n",
    "    evaluator = BinaryClassificationEvaluator(labelCol=\"Is Fraudulent\")\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    recall_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    recall = recall_eval.evaluate(predictions)\n",
    "    \n",
    "    print(f\"AUC: {auc:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error during training: {str(e)}\")\n",
    "    \n",
    "    # Let's examine the DataFrame structure\n",
    "    print(\"\\nExamining DataFrame schema:\")\n",
    "    train.printSchema()\n",
    "    \n",
    "    # Count the number of features to ensure we don't have too many\n",
    "    print(f\"\\nNumber of categorical features: {len(categorical_features)}\")\n",
    "    print(f\"Number of numerical features: {len(numerical_features)}\")\n",
    "    print(f\"Total features: {len(categorical_features) + len(numerical_features)}\")\n",
    "    \n",
    "    # Display a sample of the categorical and numerical features\n",
    "    print(\"\\nSample of feature columns:\")\n",
    "    if train.count() > 0:\n",
    "        sample_row = train.limit(1)\n",
    "        for col_name in categorical_cols[:2]:\n",
    "            print(f\"{col_name}: {sample_row.select(col_name).collect()[0][0]}\")\n",
    "        for col_name in numerical_cols[:2]:\n",
    "            print(f\"{col_name}: {sample_row.select(col_name).collect()[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a620cf-15b9-4e73-ba76-fe6ed0fe1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Check class distribution in splits\n",
    "train_fraud_ratio = train.filter(col(\"Is Fraudulent\") == 1).count() / train.count()\n",
    "test_fraud_ratio = test.filter(col(\"Is Fraudulent\") == 1).count() / test.count()\n",
    "print(f\"Train fraud ratio: {train_fraud_ratio:.2%}\")\n",
    "print(f\"Test fraud ratio: {test_fraud_ratio:.2%}\")\n",
    "\n",
    "# Instead of using CrossValidator, let's manually try different parameter combinations\n",
    "rf_params = [\n",
    "    {\"numTrees\": 50, \"maxDepth\": 5, \"minInstancesPerNode\": 1, \"impurity\": \"gini\"},\n",
    "    {\"numTrees\": 100, \"maxDepth\": 10, \"minInstancesPerNode\": 2, \"impurity\": \"entropy\"},\n",
    "    {\"numTrees\": 200, \"maxDepth\": 15, \"minInstancesPerNode\": 4, \"impurity\": \"gini\"}\n",
    "]\n",
    "\n",
    "rf_results = []\n",
    "evaluator = BinaryClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"areaUnderROC\")\n",
    "\n",
    "print(\"\\nTraining Random Forest with multiple parameter combinations...\")\n",
    "for params in rf_params:\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create Random Forest with specific parameters\n",
    "    rf = RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        weightCol=\"classWeight\",\n",
    "        **params\n",
    "    )\n",
    "    \n",
    "    # Create pipeline with preprocessing stages plus classifier\n",
    "    rf_pipeline = Pipeline(stages=stages + [rf])\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = rf_pipeline.fit(train)\n",
    "    predictions = rf_model.transform(test)\n",
    "    \n",
    "    # Evaluate\n",
    "    auc = evaluator.evaluate(predictions)\n",
    "    recall_eval = MulticlassClassificationEvaluator(\n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        metricName=\"weightedRecall\"\n",
    "    )\n",
    "    recall = recall_eval.evaluate(predictions)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    rf_results.append({\n",
    "        \"numTrees\": params[\"numTrees\"],\n",
    "        \"maxDepth\": params[\"maxDepth\"],\n",
    "        \"minInstancesPerNode\": params[\"minInstancesPerNode\"],\n",
    "        \"impurity\": params[\"impurity\"],\n",
    "        \"AUC\": auc,\n",
    "        \"Recall\": recall,\n",
    "        \"Time\": training_time\n",
    "    })\n",
    "    \n",
    "    print(f\"RF with {params} - AUC: {auc:.4f}, Recall: {recall:.4f}, Time: {training_time:.1f}s\")\n",
    "\n",
    "# Find best model\n",
    "best_rf_params = max(rf_results, key=lambda x: x[\"AUC\"])\n",
    "print(\"\\nBest Random Forest parameters:\")\n",
    "print(f\"Number of trees: {best_rf_params['numTrees']}\")\n",
    "print(f\"Max depth: {best_rf_params['maxDepth']}\")\n",
    "print(f\"Min instances per node: {best_rf_params['minInstancesPerNode']}\")\n",
    "print(f\"Impurity: {best_rf_params['impurity']}\")\n",
    "print(f\"AUC: {best_rf_params['AUC']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191b5fb6-6dba-41c2-88ce-321d08efd447",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define models to test\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        weightCol=\"classWeight\",\n",
    "        maxIter=20,\n",
    "        regParam=0.01,\n",
    "        elasticNetParam=0.5\n",
    "    ),\n",
    "    \"Random Forest\": RandomForestClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        weightCol=\"classWeight\",\n",
    "        numTrees=best_rf_model.getNumTrees(),\n",
    "        maxDepth=best_rf_model.getMaxDepth(),\n",
    "        minInstancesPerNode=best_rf_model.getMinInstancesPerNode(),\n",
    "        impurity=best_rf_model.getImpurity()\n",
    "    ),\n",
    "    \"Gradient-Boosted Trees\": GBTClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        weightCol=\"classWeight\",\n",
    "        maxIter=50,\n",
    "        maxDepth=8,\n",
    "        stepSize=0.1\n",
    "    ),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(\n",
    "        featuresCol=\"features\", \n",
    "        labelCol=\"Is Fraudulent\", \n",
    "        weightCol=\"classWeight\",\n",
    "        maxDepth=10,\n",
    "        impurity=\"entropy\"\n",
    "    )\n",
    "}\n",
    "\n",
    "# Metrics to evaluate\n",
    "metrics = {\n",
    "    \"AUC\": BinaryClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"areaUnderROC\"),\n",
    "    \"PR AUC\": BinaryClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"areaUnderPR\"),\n",
    "    \"Recall\": MulticlassClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"weightedRecall\"),\n",
    "    \"Precision\": MulticlassClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"weightedPrecision\"),\n",
    "    \"F1\": MulticlassClassificationEvaluator(labelCol=\"Is Fraudulent\", metricName=\"f1\")\n",
    "}\n",
    "\n",
    "results = []\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, model in models.items():\n",
    "    print(f\"\\nTraining {model_name}...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Create pipeline with preprocessing stages plus classifier\n",
    "    pipeline = Pipeline(stages=stages + [model])\n",
    "    \n",
    "    # Fit model\n",
    "    trained_model = pipeline.fit(train)\n",
    "    predictions = trained_model.transform(test)\n",
    "    \n",
    "    # Evaluate with all metrics\n",
    "    model_results = {\"Model\": model_name}\n",
    "    for metric_name, evaluator in metrics.items():\n",
    "        score = evaluator.evaluate(predictions)\n",
    "        model_results[metric_name] = score\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    model_results[\"Time (s)\"] = training_time\n",
    "    \n",
    "    results.append(model_results)\n",
    "    \n",
    "    # Print model performance\n",
    "    print(f\"{model_name} evaluation:\")\n",
    "    for metric, value in model_results.items():\n",
    "        if metric != \"Model\" and metric != \"Time (s)\":\n",
    "            print(f\"  - {metric}: {value:.4f}\")\n",
    "    print(f\"  - Training time: {training_time:.1f}s\")\n",
    "\n",
    "# Show results table\n",
    "results_df = spark.createDataFrame(results).orderBy(\"F1\", ascending=False)\n",
    "print(\"\\nModel Performance Comparison:\")\n",
    "results_df.show(truncate=False)\n",
    "\n",
    "# Convert to pandas for visualization\n",
    "results_pd = results_df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ef2c5-df16-4d4f-a23c-b412db37085e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot model comparison\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Set figure size\n",
    "plt.figure(figsize=(14, 8))\n",
    "\n",
    "# Get models and metrics\n",
    "models = results_pd['Model'].tolist()\n",
    "metrics_to_plot = ['AUC', 'PR AUC', 'Recall', 'Precision', 'F1']\n",
    "\n",
    "# Set width of bars\n",
    "barWidth = 0.15\n",
    "r = range(len(models))\n",
    "\n",
    "# Plot bars\n",
    "for i, metric in enumerate(metrics_to_plot):\n",
    "    plt.bar([x + i*barWidth for x in r], results_pd[metric], width=barWidth, label=metric)\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Models', fontweight='bold', fontsize=12)\n",
    "plt.ylabel('Score', fontweight='bold', fontsize=12)\n",
    "plt.xticks([r + barWidth*2 for r in range(len(models))], models, rotation=45)\n",
    "plt.legend()\n",
    "plt.title('Model Performance Comparison', fontsize=14)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n",
    "\n",
    "# Identify best model based on F1 score\n",
    "best_model_row = results_pd.loc[results_pd['F1'].idxmax()]\n",
    "print(f\"\\nBest model based on F1 score: {best_model_row['Model']}\")\n",
    "print(f\"F1 score: {best_model_row['F1']:.4f}\")\n",
    "print(f\"AUC: {best_model_row['AUC']:.4f}\")\n",
    "print(f\"Recall: {best_model_row['Recall']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ce24a1-210c-472b-ab2b-6bd9326eeeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_results_list = [r for r in results if r[\"Model\"] == rf_name]\n",
    "if rf_results_list:\n",
    "    rf_results = rf_results_list[0]\n",
    "    print(f\"\\nAnalyzing {rf_name} model (F1: {rf_results['F1']:.4f})\")\n",
    "else:\n",
    "    print(f\"Model '{rf_name}' not found in results.\")\n",
    "\n",
    "# Get the trained Random Forest model from pipeline\n",
    "rf_pipeline = Pipeline(stages=stages + [models[rf_name]])\n",
    "rf_trained = rf_pipeline.fit(train)\n",
    "rf_model = rf_trained.stages[-1]\n",
    "\n",
    "# Get feature importance\n",
    "feature_importances = rf_model.featureImportances\n",
    "print(\"Feature importances:\", feature_importances)\n",
    "\n",
    "# Get feature names (the order matches the VectorAssembler's inputCols)\n",
    "feature_cols = assembler_inputs\n",
    "\n",
    "# Create a DataFrame with feature names and importances\n",
    "if len(feature_cols) == len(feature_importances):\n",
    "    importance_list = [(feature_cols[i], float(feature_importances[i])) \n",
    "                     for i in range(len(feature_cols))]\n",
    "    \n",
    "    # Convert to Spark DataFrame and then to Pandas\n",
    "    importance_df = spark.createDataFrame(importance_list, [\"feature\", \"importance\"])\n",
    "    importance_pd = importance_df.orderBy(\"importance\", ascending=False).toPandas()\n",
    "    \n",
    "    # Plot top 15 features\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    top_n = min(15, len(importance_pd))\n",
    "    plt.barh(importance_pd['feature'][:top_n], importance_pd['importance'][:top_n])\n",
    "    plt.xlabel('Importance')\n",
    "    plt.ylabel('Feature')\n",
    "    plt.title('Top 15 Important Features')\n",
    "    plt.gca().invert_yaxis()  # To have the highest importance at the top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"Warning: Feature count mismatch. Features: {len(feature_cols)}, Importances: {len(feature_importances)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "938ef9df-cbfb-4f25-82cd-cd28fdbda48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql import SparkSession\n",
    "# from pyspark.sql.types import StructType, StructField, StringType, FloatType, IntegerType\n",
    "# from pyspark.sql.functions import col, to_date, dayofmonth, dayofweek, month, when, lit, isnull, count\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "# from pyspark.ml.classification import RandomForestClassifier\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# from pyspark.ml import Pipeline\n",
    "# from datetime import datetime\n",
    "# import time\n",
    "\n",
    "# # Initialize Spark Session with optimized configurations\n",
    "# spark = SparkSession.builder \\\n",
    "#     .appName(\"FraudDetection\") \\\n",
    "#     .master(\"local[*]\") \\\n",
    "#     .config(\"spark.sql.debug.maxToStringFields\", \"100\") \\\n",
    "#     .config(\"spark.driver.memory\", \"4g\") \\\n",
    "#     .getOrCreate()\n",
    "\n",
    "# print(\"Spark session created successfully!\")\n",
    "\n",
    "# # Define schema\n",
    "# schema = StructType([\n",
    "#     StructField(\"Transaction ID\", StringType(), True),\n",
    "#     StructField(\"Customer ID\", StringType(), True),\n",
    "#     StructField(\"Transaction Amount\", FloatType(), True),\n",
    "#     StructField(\"Transaction Date\", StringType(), True),\n",
    "#     StructField(\"Payment Method\", StringType(), True),\n",
    "#     StructField(\"Product Category\", StringType(), True),\n",
    "#     StructField(\"Quantity\", IntegerType(), True),\n",
    "#     StructField(\"Customer Age\", IntegerType(), True),\n",
    "#     StructField(\"Customer Location\", StringType(), True),\n",
    "#     StructField(\"Device Used\", StringType(), True),\n",
    "#     StructField(\"IP Address\", StringType(), True),\n",
    "#     StructField(\"Shipping Address\", StringType(), True),\n",
    "#     StructField(\"Billing Address\", StringType(), True),\n",
    "#     StructField(\"Is Fraudulent\", IntegerType(), True),\n",
    "#     StructField(\"Account Age Days\", IntegerType(), True),\n",
    "#     StructField(\"Transaction Hour\", IntegerType(), True)\n",
    "# ])\n",
    "\n",
    "# # Load data from local file with error handling\n",
    "# try:\n",
    "#     today = datetime.today()\n",
    "#     hdfs_path = f\"hdfs://namenode:9000/user/root/transactions/YYYY={today.year}/MM={today.month:02d}/DD={today.day:02d}/transaction_data.csv\"\n",
    "    \n",
    "#     df = spark.read.option(\"header\", \"true\") \\\n",
    "#                    .option(\"multiLine\", \"true\") \\\n",
    "#                    .schema(schema) \\\n",
    "#                    .csv(hdfs_path)\n",
    "    \n",
    "#     print(\"Data loaded successfully. Row count:\", df.count())\n",
    "# except Exception as e:\n",
    "#     print(f\"Error loading data: {str(e)}\")\n",
    "#     spark.stop()\n",
    "#     exit()\n",
    "# ####################################################################################################\n",
    "# # Data Quality Check\n",
    "# print(\"\\nData Quality Check:\")\n",
    "# print(\"Null values per column:\")\n",
    "# null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "# null_counts.show(vertical=True)\n",
    "# ####################################################################################################\n",
    "# # Data Quality Check\n",
    "# print(\"\\nData Quality Check:\")\n",
    "# print(\"Null values per column:\")\n",
    "# null_counts = df.select([count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
    "# null_counts.show(vertical=True)\n",
    "# ####################################################################################################\n",
    "# # Data Preprocessing\n",
    "# print(\"\\nStarting Data Preprocessing...\")\n",
    "# start_time = time.time()\n",
    "# from pyspark.sql.functions import col, to_date, dayofmonth, dayofweek, month, when\n",
    "\n",
    "# # Convert Transaction Date and extract features\n",
    "# df = df.withColumn(\"Transaction Date\", to_date(col(\"Transaction Date\")))\n",
    "# df = df.withColumn(\"DayOfMonth\", dayofmonth(col(\"Transaction Date\")))\n",
    "# df = df.withColumn(\"DayOfWeek\", dayofweek(col(\"Transaction Date\")))\n",
    "# df = df.withColumn(\"Month\", month(col(\"Transaction Date\")))\n",
    "\n",
    "# # Create feature: is shipping different from billing\n",
    "# df = df.withColumn(\"AddressMismatch\", \n",
    "#                   when(col(\"Shipping Address\") != col(\"Billing Address\"), 1).otherwise(0))\n",
    "\n",
    "# # Create feature: transaction amount per quantity\n",
    "# df = df.withColumn(\"AmountPerQuantity\", \n",
    "#                   col(\"Transaction Amount\") / col(\"Quantity\"))\n",
    "\n",
    "# # Handle class imbalance\n",
    "# fraud_count = df.filter(col(\"Is Fraudulent\") == 1).count()\n",
    "# non_fraud_count = df.filter(col(\"Is Fraudulent\") == 0).count()\n",
    "# fraud_ratio = fraud_count / (fraud_count + non_fraud_count)\n",
    "\n",
    "# print(f\"\\nFraudulent transactions: {fraud_count} ({fraud_ratio:.2%})\")\n",
    "# print(f\"Non-fraudulent transactions: {non_fraud_count}\")\n",
    "# ####################################################################################################\n",
    "# # 1. Calculate class weights (to handle imbalance)\n",
    "# fraud_weight = non_fraud_count / (fraud_count + non_fraud_count)  # Weight for fraud class\n",
    "# non_fraud_weight = fraud_count / (fraud_count + non_fraud_count)  # Weight for non-fraud\n",
    "\n",
    "# df = df.withColumn(\"classWeight\", \n",
    "#                   when(col(\"Is Fraudulent\") == 1, fraud_weight)\n",
    "#                   .otherwise(non_fraud_weight))\n",
    "\n",
    "# # 2. Fill nulls in categorical columns (defensive programming)\n",
    "# categorical_cols = [\"Payment Method\", \"Product Category\", \"Customer Location\", \"Device Used\"]\n",
    "# for col_name in categorical_cols:\n",
    "#     df = df.fillna(\"unknown\", subset=[col_name])\n",
    "\n",
    "# print(\"\\nClass weights applied and categorical nulls handled (if any existed).\")\n",
    "# print(\"Sample of weights (fraud cases should have higher weight):\")\n",
    "# df.select(\"Is Fraudulent\", \"classWeight\").show(5)\n",
    "# ####################################################################################################\n",
    "# from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler, StandardScaler, Imputer\n",
    "\n",
    "# # Define pipeline stages\n",
    "# stages = []\n",
    "\n",
    "# # 1. Categorical encoding\n",
    "# categorical_cols = [\"Payment Method\", \"Product Category\", \"Customer Location\", \"Device Used\"]\n",
    "# for col_name in categorical_cols:\n",
    "#     # Convert strings to numerical indices\n",
    "#     string_indexer = StringIndexer(inputCol=col_name, outputCol=f\"{col_name}_Index\", handleInvalid=\"keep\")\n",
    "#     # One-hot encode indices\n",
    "#     encoder = OneHotEncoder(inputCols=[f\"{col_name}_Index\"], outputCols=[f\"{col_name}_OHE\"], handleInvalid=\"keep\")\n",
    "#     stages += [string_indexer, encoder]\n",
    "\n",
    "# # 2. Numerical columns (impute missing values with median)\n",
    "# numerical_cols = [\"Transaction Amount\", \"Quantity\", \"Customer Age\", \n",
    "#                  \"Account Age Days\", \"Transaction Hour\", \"DayOfMonth\",\n",
    "#                  \"DayOfWeek\", \"Month\", \"AddressMismatch\", \"AmountPerQuantity\"]\n",
    "\n",
    "# for num_col in numerical_cols:\n",
    "#     imputer = Imputer(inputCol=num_col, outputCol=f\"{num_col}_imputed\", strategy=\"median\")\n",
    "#     stages.append(imputer)\n",
    "#     numerical_cols[numerical_cols.index(num_col)] = f\"{num_col}_imputed\"  # Update column name\n",
    "\n",
    "# # 3. Assemble all features into a vector\n",
    "# assembler_inputs = [f\"{c}_OHE\" for c in categorical_cols] + numerical_cols\n",
    "# assembler = VectorAssembler(inputCols=assembler_inputs, outputCol=\"rawFeatures\", handleInvalid=\"keep\")\n",
    "# stages.append(assembler)\n",
    "\n",
    "# # 4. Scale features (mean=0, std=1)\n",
    "# scaler = StandardScaler(inputCol=\"rawFeatures\", outputCol=\"features\", withStd=True, withMean=True)\n",
    "# stages.append(scaler)\n",
    "\n",
    "# # 5. Random Forest with class weights\n",
    "# rf = RandomForestClassifier(\n",
    "#     featuresCol=\"features\",\n",
    "#     labelCol=\"Is Fraudulent\",\n",
    "#     weightCol=\"classWeight\",  # Critical for imbalance!\n",
    "#     numTrees=50,\n",
    "#     maxDepth=5,\n",
    "#     seed=42\n",
    "# )\n",
    "# stages.append(rf)\n",
    "\n",
    "# # Create the pipeline\n",
    "# pipeline = Pipeline(stages=stages)\n",
    "# print(\"\\nPipeline built successfully. Ready for training!\")\n",
    "# ####################################################################################################\n",
    "# # Split data (80% train, 20% test)\n",
    "# train, test = df.randomSplit([0.8, 0.2], seed=42)\n",
    "# print(f\"\\nTrain rows: {train.count()}, Test rows: {test.count()}\")\n",
    "\n",
    "# # Train the model\n",
    "# print(\"\\nTraining started...\")\n",
    "# start_time = time.time()\n",
    "# model = pipeline.fit(train)\n",
    "# print(f\"Training completed in {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# # Generate predictions on test set\n",
    "# predictions = model.transform(test)\n",
    "# print(\"\\nPredictions ready for evaluation.\")\n",
    "# ####################################################################################################\n",
    "# from pyspark.ml.classification import (\n",
    "#     LogisticRegression, \n",
    "#     RandomForestClassifier, \n",
    "#     GBTClassifier, \n",
    "#     DecisionTreeClassifier\n",
    "# )\n",
    "# from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "# from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "# import time\n",
    "\n",
    "# # Define models to test\n",
    "# models = {\n",
    "#     \"Logistic Regression\": LogisticRegression(\n",
    "#         featuresCol=\"features\", \n",
    "#         labelCol=\"Is Fraudulent\", \n",
    "#         weightCol=\"classWeight\"\n",
    "#     ),\n",
    "#     \"Random Forest\": RandomForestClassifier(\n",
    "#         featuresCol=\"features\", \n",
    "#         labelCol=\"Is Fraudulent\", \n",
    "#         weightCol=\"classWeight\",\n",
    "#         numTrees=50\n",
    "#     ),\n",
    "#     \"Gradient-Boosted Trees\": GBTClassifier(\n",
    "#         featuresCol=\"features\", \n",
    "#         labelCol=\"Is Fraudulent\", \n",
    "#         weightCol=\"classWeight\",\n",
    "#         maxIter=20\n",
    "#     ),\n",
    "#     \"Decision Tree\": DecisionTreeClassifier(\n",
    "#         featuresCol=\"features\", \n",
    "#         labelCol=\"Is Fraudulent\", \n",
    "#         weightCol=\"classWeight\"\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# # Metrics to evaluate\n",
    "# metrics = {\n",
    "#     \"AUC\": BinaryClassificationEvaluator(labelCol=\"Is Fraudulent\"),\n",
    "#     \"Recall\": MulticlassClassificationEvaluator(\n",
    "#         labelCol=\"Is Fraudulent\", \n",
    "#         metricName=\"weightedRecall\"\n",
    "#     )\n",
    "# }\n",
    "\n",
    "# results = []\n",
    "\n",
    "# # Train and evaluate each model\n",
    "# for model_name, model in models.items():\n",
    "#     print(f\"\\nTraining {model_name}...\")\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     # Create pipeline (reuse your existing stages, replace classifier)\n",
    "#     pipeline = Pipeline(stages=stages[:-1] + [model])  # Keep all but last stage\n",
    "    \n",
    "#     # Fit model\n",
    "#     trained_model = pipeline.fit(train)\n",
    "#     predictions = trained_model.transform(test)\n",
    "    \n",
    "#     # Evaluate\n",
    "#     auc = metrics[\"AUC\"].evaluate(predictions)\n",
    "#     recall = metrics[\"Recall\"].evaluate(predictions)\n",
    "#     training_time = time.time() - start_time\n",
    "    \n",
    "#     results.append({\n",
    "#         \"Model\": model_name,\n",
    "#         \"AUC\": auc,\n",
    "#         \"Recall\": recall,\n",
    "#         \"Time (s)\": training_time\n",
    "#     })\n",
    "    \n",
    "#     print(f\"{model_name} - AUC: {auc:.4f}, Recall: {recall:.4f}, Time: {training_time:.1f}s\")\n",
    "\n",
    "# # Show results\n",
    "# results_df = spark.createDataFrame(results).orderBy(\"Recall\", ascending=False)\n",
    "# print(\"\\nModel Performance Comparison:\")\n",
    "# results_df.show(truncate=False)\n",
    "# ####################################################################################################\n",
    "# spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c67d12f5-c01b-47fa-b52a-b195ff1b40fa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
