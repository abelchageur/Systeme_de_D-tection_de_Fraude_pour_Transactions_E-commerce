[2025-05-24T13:29:18.808+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command manual__2025-05-24T13:28:40.655570+00:00 [queued]>
[2025-05-24T13:29:18.824+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command manual__2025-05-24T13:28:40.655570+00:00 [queued]>
[2025-05-24T13:29:18.825+0000] {taskinstance.py:1361} INFO - Starting attempt 1 of 2
[2025-05-24T13:29:18.849+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_spark_modelisation_command> on 2025-05-24 13:28:40.655570+00:00
[2025-05-24T13:29:18.860+0000] {standard_task_runner.py:57} INFO - Started process 252 to run task
[2025-05-24T13:29:18.871+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'fetch_transaction_data_and_upload_to_hdfs', 'run_spark_modelisation_command', 'manual__2025-05-24T13:28:40.655570+00:00', '--job-id', '6', '--raw', '--subdir', 'DAGS_FOLDER/ETL.py', '--cfg-path', '/tmp/tmpkcolsasr']
[2025-05-24T13:29:18.875+0000] {standard_task_runner.py:85} INFO - Job 6: Subtask run_spark_modelisation_command
[2025-05-24T13:29:18.965+0000] {task_command.py:415} INFO - Running <TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command manual__2025-05-24T13:28:40.655570+00:00 [running]> on host 1be44660cb93
[2025-05-24T13:29:19.405+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='fetch_transaction_data_and_upload_to_hdfs' AIRFLOW_CTX_TASK_ID='run_spark_modelisation_command' AIRFLOW_CTX_EXECUTION_DATE='2025-05-24T13:28:40.655570+00:00' AIRFLOW_CTX_TRY_NUMBER='1' AIRFLOW_CTX_DAG_RUN_ID='manual__2025-05-24T13:28:40.655570+00:00'
[2025-05-24T13:29:19.407+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-24T13:29:19.409+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-master python3 modelisation.py']
[2025-05-24T13:29:19.431+0000] {subprocess.py:86} INFO - Output:
[2025-05-24T13:29:34.050+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2025-05-24T13:29:34.051+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-05-24T13:29:34.589+0000] {subprocess.py:93} INFO - 25/05/24 13:29:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-24T13:29:35.698+0000] {subprocess.py:93} INFO - 25/05/24 13:29:35 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
[2025-05-24T13:29:43.600+0000] {subprocess.py:93} INFO - [Stage 0:>                                                          (0 + 1) / 1]25/05/24 13:29:43 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-24T13:29:43.601+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-24T13:29:43.603+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-24T13:29:43.604+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-24T13:29:43.605+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=24/transaction_data.csv
[2025-05-24T13:29:47.255+0000] {subprocess.py:93} INFO - [Stage 0:===========================================================(1 + 0) / 1]                                                                                25/05/24 13:29:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-24T13:29:47.258+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-24T13:29:47.260+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-24T13:29:47.262+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-24T13:29:47.263+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=24/transaction_data.csv
[2025-05-24T13:29:47.307+0000] {subprocess.py:93} INFO - 25/05/24 13:29:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-24T13:29:47.308+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-24T13:29:47.309+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-24T13:29:47.314+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-24T13:29:47.318+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=24/transaction_data.csv
[2025-05-24T13:29:47.592+0000] {subprocess.py:93} INFO - [Stage 3:>                                                          (0 + 1) / 1][Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1]25/05/24 13:29:47 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-24T13:29:47.593+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-24T13:29:47.595+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-24T13:29:47.597+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-24T13:29:47.598+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=24/transaction_data.csv
[2025-05-24T13:29:52.328+0000] {subprocess.py:93} INFO - [Stage 3:>    (0 + 1) / 1][Stage 4:>    (0 + 1) / 1][Stage 5:>    (0 + 1) / 1][Stage 7:>                                                          (0 + 1) / 1]                                                                                25/05/24 13:29:52 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-24T13:29:52.329+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-24T13:29:52.330+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-24T13:29:52.331+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-24T13:29:52.333+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=24/transaction_data.csv
[2025-05-24T13:29:52.844+0000] {subprocess.py:93} INFO - [Stage 12:>                                                         (0 + 1) / 1]25/05/24 13:29:52 WARN DataStreamer: DataStreamer Exception
[2025-05-24T13:29:52.845+0000] {subprocess.py:93} INFO - java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:52.846+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:52.846+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:52.847+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:52.848+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:52.849+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:52.850+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:52.851+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:52.851+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:52.852+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:52.853+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:52.854+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:52.854+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:52.855+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:52.856+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:52.856+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:52.857+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:52.857+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:52.858+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:52.859+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:52.859+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:52.860+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:52.860+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:52.863+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:52.865+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:52.866+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:52.866+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:52.867+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:52.868+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:52.868+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:52.869+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:52.869+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:52.870+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:52.871+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:52.872+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:52.874+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:52.875+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:52.877+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:52.877+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:52.879+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:52.880+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:52.881+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:52.882+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:52.883+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:52.884+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:52.884+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:52.885+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:52.885+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:52.886+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:52.886+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:52.889+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:52.889+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:52.890+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:52.891+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:52.892+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:52.893+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:52.894+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:52.895+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:52.897+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:52.897+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:52.898+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:52.899+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:52.900+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:52.902+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:52.903+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 ERROR Utils: Aborting task
[2025-05-24T13:29:52.905+0000] {subprocess.py:93} INFO - com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:52.907+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:52.908+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:52.909+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:52.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:52.912+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:52.913+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:52.915+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:52.916+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:52.917+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:52.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:52.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:52.919+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:52.920+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:52.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:52.921+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:52.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:52.922+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:52.923+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:52.924+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:52.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:52.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:52.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:52.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:52.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:52.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:52.929+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:52.930+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:52.930+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:52.931+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:52.932+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:52.933+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:52.934+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:52.935+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:52.936+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:52.938+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:52.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:52.939+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:52.942+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:52.943+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:52.944+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:52.945+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:52.946+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:52.947+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:52.947+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:52.948+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:52.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:52.950+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:52.951+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:52.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:52.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:52.953+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:52.954+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:52.954+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:52.955+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:52.956+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:52.956+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:52.957+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:52.958+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:52.958+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:52.959+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:52.960+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:52.960+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:52.961+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:52.961+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:52.962+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:52.962+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:52.963+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:52.964+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:52.964+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:52.965+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:52.966+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:52.966+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:52.967+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:52.967+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:52.968+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:52.969+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:52.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:52.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:52.973+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:52.975+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:52.976+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:52.977+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:52.978+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:52.979+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:52.980+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:52.981+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:52.983+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:52.984+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:52.987+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:52.990+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:52.992+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:52.993+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:52.994+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:52.994+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:52.995+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:52.996+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:52.998+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:52.999+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.000+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 WARN FileOutputCommitter: Could not delete hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8
[2025-05-24T13:29:53.000+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 ERROR FileFormatWriter: Job job_202505241329515731155947799130269_0012 aborted.
[2025-05-24T13:29:53.001+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 ERROR Executor: Exception in task 0.0 in stage 12.0 (TID 8)
[2025-05-24T13:29:53.002+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.003+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.004+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.005+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.006+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.007+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.008+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.009+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.010+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.011+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.012+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.013+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.016+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.020+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.022+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.023+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.024+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.024+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.025+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.026+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.027+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.028+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.029+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.031+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.032+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.034+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.035+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.036+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.036+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.037+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.038+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.039+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.040+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.040+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.041+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.042+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.043+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.044+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.044+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.045+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.046+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.046+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.047+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.047+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.050+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.053+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.054+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.055+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.055+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.056+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.057+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.057+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.058+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.059+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.060+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.060+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.061+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.062+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.063+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.063+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.064+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.065+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.066+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.066+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.067+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.068+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.069+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.070+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.072+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.074+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.075+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.077+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.078+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.079+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.081+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.081+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.082+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.083+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.083+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.084+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.085+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.085+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.087+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.088+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.089+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.089+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.090+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.091+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.091+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.092+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.093+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.093+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.094+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.095+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.096+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.097+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.098+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.098+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.099+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.100+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 WARN TaskSetManager: Lost task 0.0 in stage 12.0 (TID 8) (a04402f3e17c executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.101+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.102+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.103+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.104+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.105+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.105+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.106+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.107+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.108+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.108+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.109+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.110+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.110+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.111+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.112+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.112+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.113+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.113+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.114+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.115+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.116+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.116+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.117+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.118+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.119+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.120+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.121+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.122+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.123+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.123+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.124+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.124+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.125+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.125+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.126+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.126+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.127+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.128+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.128+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.129+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.130+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.130+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.131+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.132+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.132+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.133+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.133+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.134+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.134+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.135+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.135+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.135+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.136+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.136+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.137+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.137+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.137+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.138+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.138+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.139+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.139+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.140+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.140+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.141+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.142+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.142+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.143+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.143+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.144+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.145+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.146+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.147+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.148+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.149+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.149+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.150+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.150+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.151+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.152+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.152+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.153+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.154+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.155+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.155+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.156+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.156+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.157+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.158+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.161+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.163+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.164+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.166+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.168+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.171+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.173+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.175+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.177+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.179+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.179+0000] {subprocess.py:93} INFO - 25/05/24 13:29:52 ERROR TaskSetManager: Task 0 in stage 12.0 failed 1 times; aborting job
[2025-05-24T13:29:53.180+0000] {subprocess.py:93} INFO - 25/05/24 13:29:53 ERROR FileFormatWriter: Aborting job fe90a1ae-624e-482a-835c-a6b9a248141a.
[2025-05-24T13:29:53.181+0000] {subprocess.py:93} INFO - org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 8) (a04402f3e17c executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.182+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.184+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.185+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.186+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.189+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.190+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.191+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.195+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.201+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.203+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.204+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.206+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.207+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.208+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.209+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.210+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.211+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.211+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.212+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.215+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.220+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.221+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.223+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.225+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.226+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.227+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.229+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.230+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.231+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.245+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.247+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.249+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.250+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.251+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.252+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.253+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.254+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.254+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.255+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.256+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.257+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.258+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.259+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.262+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.265+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.266+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.267+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.268+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.269+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.270+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.270+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.271+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.272+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.272+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.273+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.274+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.274+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.275+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.275+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.276+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.277+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.277+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.278+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.279+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.280+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.281+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.282+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.283+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.284+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.284+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.285+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.286+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.286+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.287+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.288+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.289+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.289+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.290+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.291+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.291+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.292+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.293+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.294+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.295+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.295+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.296+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.297+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.297+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.298+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.299+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.299+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.300+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.301+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.301+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.302+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.303+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.303+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.304+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.305+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.305+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-05-24T13:29:53.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-05-24T13:29:53.306+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-05-24T13:29:53.307+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-05-24T13:29:53.308+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-05-24T13:29:53.308+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-05-24T13:29:53.309+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-05-24T13:29:53.309+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-05-24T13:29:53.310+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.311+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.311+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-05-24T13:29:53.312+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.312+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-05-24T13:29:53.313+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-05-24T13:29:53.313+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-05-24T13:29:53.314+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-05-24T13:29:53.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-05-24T13:29:53.315+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2025-05-24T13:29:53.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2025-05-24T13:29:53.316+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2025-05-24T13:29:53.317+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2025-05-24T13:29:53.317+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2025-05-24T13:29:53.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2025-05-24T13:29:53.318+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-05-24T13:29:53.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-05-24T13:29:53.319+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-05-24T13:29:53.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)
[2025-05-24T13:29:53.320+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)
[2025-05-24T13:29:53.321+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)
[2025-05-24T13:29:53.321+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2025-05-24T13:29:53.322+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-05-24T13:29:53.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-05-24T13:29:53.323+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-05-24T13:29:53.324+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-24T13:29:53.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-05-24T13:29:53.325+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2025-05-24T13:29:53.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-05-24T13:29:53.326+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2025-05-24T13:29:53.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2025-05-24T13:29:53.327+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2025-05-24T13:29:53.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.328+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-05-24T13:29:53.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-05-24T13:29:53.329+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.330+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2025-05-24T13:29:53.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2025-05-24T13:29:53.331+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2025-05-24T13:29:53.332+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2025-05-24T13:29:53.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2025-05-24T13:29:53.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
[2025-05-24T13:29:53.333+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
[2025-05-24T13:29:53.334+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
[2025-05-24T13:29:53.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
[2025-05-24T13:29:53.335+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)
[2025-05-24T13:29:53.336+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.336+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.336+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.337+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.338+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-05-24T13:29:53.338+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-24T13:29:53.339+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-05-24T13:29:53.339+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-05-24T13:29:53.340+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-05-24T13:29:53.340+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-24T13:29:53.341+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-24T13:29:53.342+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.342+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.343+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.344+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.344+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.345+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.346+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.347+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.348+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.349+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.350+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.351+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.351+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.352+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.352+0000] {subprocess.py:93} INFO - 	... 1 more
[2025-05-24T13:29:53.353+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.353+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.354+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.355+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.355+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.356+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.357+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.358+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.359+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.360+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.360+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.361+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.361+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.362+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.362+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.363+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.363+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.364+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.364+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.365+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.366+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.366+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.367+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.367+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.368+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.368+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.369+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.369+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.370+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.371+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.371+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.372+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.372+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.373+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.374+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.375+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.375+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.376+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.377+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.378+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.379+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.380+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.381+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.382+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.382+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.383+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.384+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.384+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.385+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.385+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.386+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.387+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.387+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.388+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.389+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.389+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.390+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.391+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.392+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.392+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.393+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.393+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.394+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.394+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.395+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.396+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.396+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.397+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.397+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.398+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.398+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.399+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.400+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.400+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.401+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.402+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.402+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.403+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.403+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.404+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.703+0000] {subprocess.py:93} INFO - Spark session created successfully!
[2025-05-24T13:29:53.704+0000] {subprocess.py:93} INFO - Data loaded successfully. Row count: 23634
[2025-05-24T13:29:53.720+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-24T13:29:53.721+0000] {subprocess.py:93} INFO -   File "/opt/spark/work-dir/modelisation.py", line 318, in <module>
[2025-05-24T13:29:53.731+0000] {subprocess.py:93} INFO -     .csv(fact_path)
[2025-05-24T13:29:53.732+0000] {subprocess.py:93} INFO -      ^^^^^^^^^^^^^^
[2025-05-24T13:29:53.733+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/pyspark/sql/readwriter.py", line 1864, in csv
[2025-05-24T13:29:53.734+0000] {subprocess.py:93} INFO -     self._jwrite.csv(path)
[2025-05-24T13:29:53.735+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-24T13:29:53.737+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py", line 179, in deco
[2025-05-24T13:29:53.738+0000] {subprocess.py:93} INFO -     return f(*a, **kw)
[2025-05-24T13:29:53.739+0000] {subprocess.py:93} INFO -            ^^^^^^^^^^^
[2025-05-24T13:29:53.740+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j.zip/py4j/protocol.py", line 326, in get_return_value
[2025-05-24T13:29:53.783+0000] {subprocess.py:93} INFO - py4j.protocol.Py4JJavaError: An error occurred while calling o173.csv.
[2025-05-24T13:29:53.784+0000] {subprocess.py:93} INFO - : org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 12.0 failed 1 times, most recent failure: Lost task 0.0 in stage 12.0 (TID 8) (a04402f3e17c executor driver): org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.785+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.786+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.787+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.788+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.789+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.790+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.791+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.792+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.793+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.794+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.795+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.796+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.796+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.797+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.797+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.798+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.799+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.799+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.800+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.802+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.804+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.804+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.805+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.806+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.807+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.808+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.809+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.810+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.811+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.811+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.812+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.814+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.816+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.818+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.819+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.820+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.821+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.821+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.822+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.822+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.823+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.824+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.825+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.825+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.826+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.826+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.827+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.828+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.829+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.830+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.830+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.831+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.832+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.832+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.833+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.833+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.834+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.834+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.835+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.836+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.836+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.837+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.838+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.838+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.839+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.840+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.840+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.841+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.842+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.842+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.843+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.844+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.845+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.846+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.846+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.847+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.847+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.848+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.850+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.850+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.851+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.852+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.853+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.854+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.854+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.855+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.855+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.856+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.857+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.858+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.859+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.859+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.860+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.861+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.861+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.862+0000] {subprocess.py:93} INFO - Driver stacktrace:
[2025-05-24T13:29:53.862+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)
[2025-05-24T13:29:53.863+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)
[2025-05-24T13:29:53.864+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)
[2025-05-24T13:29:53.864+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)
[2025-05-24T13:29:53.865+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)
[2025-05-24T13:29:53.866+0000] {subprocess.py:93} INFO - 	at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)
[2025-05-24T13:29:53.867+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)
[2025-05-24T13:29:53.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.868+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.869+0000] {subprocess.py:93} INFO - 	at scala.Option.foreach(Option.scala:407)
[2025-05-24T13:29:53.870+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)
[2025-05-24T13:29:53.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)
[2025-05-24T13:29:53.871+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)
[2025-05-24T13:29:53.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)
[2025-05-24T13:29:53.872+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)
[2025-05-24T13:29:53.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)
[2025-05-24T13:29:53.873+0000] {subprocess.py:93} INFO - 	at org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)
[2025-05-24T13:29:53.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeWrite$4(FileFormatWriter.scala:307)
[2025-05-24T13:29:53.874+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.writeAndCommit(FileFormatWriter.scala:271)
[2025-05-24T13:29:53.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeWrite(FileFormatWriter.scala:304)
[2025-05-24T13:29:53.875+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.write(FileFormatWriter.scala:190)
[2025-05-24T13:29:53.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.InsertIntoHadoopFsRelationCommand.run(InsertIntoHadoopFsRelationCommand.scala:190)
[2025-05-24T13:29:53.876+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult$lzycompute(commands.scala:113)
[2025-05-24T13:29:53.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.sideEffectResult(commands.scala:111)
[2025-05-24T13:29:53.877+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.command.DataWritingCommandExec.executeCollect(commands.scala:125)
[2025-05-24T13:29:53.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.$anonfun$executeCollect$1(AdaptiveSparkPlanExec.scala:392)
[2025-05-24T13:29:53.878+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.withFinalPlanUpdate(AdaptiveSparkPlanExec.scala:420)
[2025-05-24T13:29:53.879+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.adaptive.AdaptiveSparkPlanExec.executeCollect(AdaptiveSparkPlanExec.scala:392)
[2025-05-24T13:29:53.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.$anonfun$applyOrElse$1(QueryExecution.scala:107)
[2025-05-24T13:29:53.880+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$6(SQLExecution.scala:125)
[2025-05-24T13:29:53.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:201)
[2025-05-24T13:29:53.881+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.$anonfun$withNewExecutionId$1(SQLExecution.scala:108)
[2025-05-24T13:29:53.882+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:900)
[2025-05-24T13:29:53.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:66)
[2025-05-24T13:29:53.883+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:107)
[2025-05-24T13:29:53.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution$$anonfun$eagerlyExecuteCommands$1.applyOrElse(QueryExecution.scala:98)
[2025-05-24T13:29:53.884+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:461)
[2025-05-24T13:29:53.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(origin.scala:76)
[2025-05-24T13:29:53.885+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:461)
[2025-05-24T13:29:53.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:267)
[2025-05-24T13:29:53.886+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:263)
[2025-05-24T13:29:53.887+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.888+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:32)
[2025-05-24T13:29:53.888+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:437)
[2025-05-24T13:29:53.889+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.eagerlyExecuteCommands(QueryExecution.scala:98)
[2025-05-24T13:29:53.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted$lzycompute(QueryExecution.scala:85)
[2025-05-24T13:29:53.890+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.commandExecuted(QueryExecution.scala:83)
[2025-05-24T13:29:53.891+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.QueryExecution.assertCommandExecuted(QueryExecution.scala:142)
[2025-05-24T13:29:53.892+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:869)
[2025-05-24T13:29:53.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:391)
[2025-05-24T13:29:53.893+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.saveInternal(DataFrameWriter.scala:364)
[2025-05-24T13:29:53.894+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:243)
[2025-05-24T13:29:53.894+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.DataFrameWriter.csv(DataFrameWriter.scala:860)
[2025-05-24T13:29:53.895+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.896+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.897+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.897+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.898+0000] {subprocess.py:93} INFO - 	at py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)
[2025-05-24T13:29:53.899+0000] {subprocess.py:93} INFO - 	at py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)
[2025-05-24T13:29:53.900+0000] {subprocess.py:93} INFO - 	at py4j.Gateway.invoke(Gateway.java:282)
[2025-05-24T13:29:53.900+0000] {subprocess.py:93} INFO - 	at py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)
[2025-05-24T13:29:53.901+0000] {subprocess.py:93} INFO - 	at py4j.commands.CallCommand.execute(CallCommand.java:79)
[2025-05-24T13:29:53.902+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)
[2025-05-24T13:29:53.902+0000] {subprocess.py:93} INFO - 	at py4j.ClientServerConnection.run(ClientServerConnection.java:106)
[2025-05-24T13:29:53.903+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.Thread.run(Unknown Source)
[2025-05-24T13:29:53.903+0000] {subprocess.py:93} INFO - Caused by: org.apache.spark.SparkException: [TASK_WRITE_FAILED] Task failed while writing rows to hdfs://namenode:9000/user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24.
[2025-05-24T13:29:53.904+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.errors.QueryExecutionErrors$.taskFailedWhileWritingRowsError(QueryExecutionErrors.scala:775)
[2025-05-24T13:29:53.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:420)
[2025-05-24T13:29:53.905+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.WriteFilesExec.$anonfun$doExecuteWrite$1(WriteFiles.scala:100)
[2025-05-24T13:29:53.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2(RDD.scala:893)
[2025-05-24T13:29:53.906+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.$anonfun$mapPartitionsInternal$2$adapted(RDD.scala:893)
[2025-05-24T13:29:53.907+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:52)
[2025-05-24T13:29:53.908+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:367)
[2025-05-24T13:29:53.908+0000] {subprocess.py:93} INFO - 	at org.apache.spark.rdd.RDD.iterator(RDD.scala:331)
[2025-05-24T13:29:53.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)
[2025-05-24T13:29:53.909+0000] {subprocess.py:93} INFO - 	at org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)
[2025-05-24T13:29:53.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.scheduler.Task.run(Task.scala:141)
[2025-05-24T13:29:53.910+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)
[2025-05-24T13:29:53.911+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)
[2025-05-24T13:29:53.913+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)
[2025-05-24T13:29:53.914+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)
[2025-05-24T13:29:53.918+0000] {subprocess.py:93} INFO - 	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)
[2025-05-24T13:29:53.920+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
[2025-05-24T13:29:53.920+0000] {subprocess.py:93} INFO - 	at java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
[2025-05-24T13:29:53.921+0000] {subprocess.py:93} INFO - 	... 1 more
[2025-05-24T13:29:53.922+0000] {subprocess.py:93} INFO - Caused by: com.univocity.parsers.common.TextWritingException: Error writing row.
[2025-05-24T13:29:53.922+0000] {subprocess.py:93} INFO - Internal state when error was thrown: recordCount=1072, recordData=[e551cb85-fe4c-43cb-8339-7b319247b4af, 3436eab7-5ba4-47cc-acf3-ad74be289baf, 580.23, 4, false, 2024-01-04 04:40:12, 2, 1, 1]
[2025-05-24T13:29:53.923+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1055)
[2025-05-24T13:29:53.924+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:834)
[2025-05-24T13:29:53.925+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.catalyst.csv.UnivocityGenerator.write(UnivocityGenerator.scala:117)
[2025-05-24T13:29:53.926+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.csv.CsvOutputWriter.write(CsvOutputWriter.scala:46)
[2025-05-24T13:29:53.927+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.SingleDirectoryDataWriter.write(FileFormatDataWriter.scala:175)
[2025-05-24T13:29:53.928+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithMetrics(FileFormatDataWriter.scala:85)
[2025-05-24T13:29:53.929+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatDataWriter.writeWithIterator(FileFormatDataWriter.scala:92)
[2025-05-24T13:29:53.930+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.$anonfun$executeTask$1(FileFormatWriter.scala:403)
[2025-05-24T13:29:53.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.util.Utils$.tryWithSafeFinallyAndFailureCallbacks(Utils.scala:1397)
[2025-05-24T13:29:53.931+0000] {subprocess.py:93} INFO - 	at org.apache.spark.sql.execution.datasources.FileFormatWriter$.executeTask(FileFormatWriter.scala:410)
[2025-05-24T13:29:53.933+0000] {subprocess.py:93} INFO - 	... 17 more
[2025-05-24T13:29:53.934+0000] {subprocess.py:93} INFO - Caused by: java.lang.IllegalStateException: Error closing the output.
[2025-05-24T13:29:53.934+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.close(AbstractWriter.java:1000)
[2025-05-24T13:29:53.935+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.throwExceptionAndClose(AbstractWriter.java:1042)
[2025-05-24T13:29:53.936+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.internalWriteRow(AbstractWriter.java:949)
[2025-05-24T13:29:53.937+0000] {subprocess.py:93} INFO - 	at com.univocity.parsers.common.AbstractWriter.writeRow(AbstractWriter.java:832)
[2025-05-24T13:29:53.938+0000] {subprocess.py:93} INFO - 	... 25 more
[2025-05-24T13:29:53.939+0000] {subprocess.py:93} INFO - Caused by: java.io.FileNotFoundException: File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.940+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.941+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.942+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.942+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.943+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.943+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.944+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.945+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.949+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.950+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.952+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.953+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.953+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.954+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.955+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.956+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.957+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
[2025-05-24T13:29:53.958+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.958+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingConstructorAccessorImpl.newInstance(Unknown Source)
[2025-05-24T13:29:53.959+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstanceWithCaller(Unknown Source)
[2025-05-24T13:29:53.960+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Constructor.newInstance(Unknown Source)
[2025-05-24T13:29:53.961+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.instantiateException(RemoteException.java:121)
[2025-05-24T13:29:53.962+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RemoteException.unwrapRemoteException(RemoteException.java:88)
[2025-05-24T13:29:53.962+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1091)
[2025-05-24T13:29:53.963+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.locateFollowingBlock(DataStreamer.java:1915)
[2025-05-24T13:29:53.964+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.nextBlockOutputStream(DataStreamer.java:1717)
[2025-05-24T13:29:53.965+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:713)
[2025-05-24T13:29:53.966+0000] {subprocess.py:93} INFO - Caused by: org.apache.hadoop.ipc.RemoteException(java.io.FileNotFoundException): File does not exist: /user/root/datalake/fact_transactions/YYYY=2025/MM=05/DD=24/_temporary/0/_temporary/attempt_202505241329515731155947799130269_0012_m_000000_8/part-00000-8804ad08-d893-4808-ad5b-a58617456ebf-c000.csv (inode 30360) Holder DFSClient_NONMAPREDUCE_-1787491847_19 does not have any open files.
[2025-05-24T13:29:53.967+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:2898)
[2025-05-24T13:29:53.967+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.analyzeFileState(FSDirWriteFileOp.java:599)
[2025-05-24T13:29:53.969+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSDirWriteFileOp.validateAddBlock(FSDirWriteFileOp.java:171)
[2025-05-24T13:29:53.969+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:2777)
[2025-05-24T13:29:53.971+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.addBlock(NameNodeRpcServer.java:892)
[2025-05-24T13:29:53.972+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.addBlock(ClientNamenodeProtocolServerSideTranslatorPB.java:574)
[2025-05-24T13:29:53.973+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
[2025-05-24T13:29:53.974+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:528)
[2025-05-24T13:29:53.975+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1070)
[2025-05-24T13:29:53.976+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:999)
[2025-05-24T13:29:53.977+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:927)
[2025-05-24T13:29:53.977+0000] {subprocess.py:93} INFO - 	at java.security.AccessController.doPrivileged(Native Method)
[2025-05-24T13:29:53.978+0000] {subprocess.py:93} INFO - 	at javax.security.auth.Subject.doAs(Subject.java:422)
[2025-05-24T13:29:53.979+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)
[2025-05-24T13:29:53.980+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2915)
[2025-05-24T13:29:53.980+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:53.981+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1612)
[2025-05-24T13:29:53.981+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1558)
[2025-05-24T13:29:53.982+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.Client.call(Client.java:1455)
[2025-05-24T13:29:53.983+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:242)
[2025-05-24T13:29:53.983+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.ipc.ProtobufRpcEngine2$Invoker.invoke(ProtobufRpcEngine2.java:129)
[2025-05-24T13:29:53.984+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy37.addBlock(Unknown Source)
[2025-05-24T13:29:53.985+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.addBlock(ClientNamenodeProtocolTranslatorPB.java:530)
[2025-05-24T13:29:53.985+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
[2025-05-24T13:29:53.986+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.986+0000] {subprocess.py:93} INFO - 	at java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
[2025-05-24T13:29:53.987+0000] {subprocess.py:93} INFO - 	at java.base/java.lang.reflect.Method.invoke(Unknown Source)
[2025-05-24T13:29:53.988+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
[2025-05-24T13:29:53.989+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
[2025-05-24T13:29:53.990+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
[2025-05-24T13:29:53.991+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
[2025-05-24T13:29:53.991+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
[2025-05-24T13:29:53.992+0000] {subprocess.py:93} INFO - 	at jdk.proxy2/jdk.proxy2.$Proxy38.addBlock(Unknown Source)
[2025-05-24T13:29:53.993+0000] {subprocess.py:93} INFO - 	at org.apache.hadoop.hdfs.DFSOutputStream.addBlock(DFSOutputStream.java:1088)
[2025-05-24T13:29:53.996+0000] {subprocess.py:93} INFO - 	... 3 more
[2025-05-24T13:29:53.999+0000] {subprocess.py:93} INFO - 
[2025-05-24T13:29:54.583+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-24T13:29:54.605+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-24T13:29:54.616+0000] {taskinstance.py:1400} INFO - Marking task as UP_FOR_RETRY. dag_id=fetch_transaction_data_and_upload_to_hdfs, task_id=run_spark_modelisation_command, execution_date=20250524T132840, start_date=20250524T132918, end_date=20250524T132954
[2025-05-24T13:29:54.650+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 6 for task run_spark_modelisation_command (Bash command failed. The command returned a non-zero exit code 1.; 252)
[2025-05-24T13:29:54.690+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-24T13:29:54.740+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
