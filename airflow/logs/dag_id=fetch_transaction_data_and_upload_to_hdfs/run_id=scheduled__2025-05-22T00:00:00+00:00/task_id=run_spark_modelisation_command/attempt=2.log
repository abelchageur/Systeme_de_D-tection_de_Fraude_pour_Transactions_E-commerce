[2025-05-23T08:11:06.119+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [queued]>
[2025-05-23T08:11:06.137+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [queued]>
[2025-05-23T08:11:06.139+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-05-23T08:11:06.169+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_spark_modelisation_command> on 2025-05-22 00:00:00+00:00
[2025-05-23T08:11:06.182+0000] {standard_task_runner.py:57} INFO - Started process 12100 to run task
[2025-05-23T08:11:06.186+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'fetch_transaction_data_and_upload_to_hdfs', 'run_spark_modelisation_command', 'scheduled__2025-05-22T00:00:00+00:00', '--job-id', '30', '--raw', '--subdir', 'DAGS_FOLDER/ETL.py', '--cfg-path', '/tmp/tmplbc1v6__']
[2025-05-23T08:11:06.192+0000] {standard_task_runner.py:85} INFO - Job 30: Subtask run_spark_modelisation_command
[2025-05-23T08:11:06.270+0000] {task_command.py:415} INFO - Running <TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [running]> on host cb5b79c23ad2
[2025-05-23T08:11:06.630+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='fetch_transaction_data_and_upload_to_hdfs' AIRFLOW_CTX_TASK_ID='run_spark_modelisation_command' AIRFLOW_CTX_EXECUTION_DATE='2025-05-22T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-22T00:00:00+00:00'
[2025-05-23T08:11:06.633+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-23T08:11:06.634+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-master python3 modelisation.py']
[2025-05-23T08:11:06.656+0000] {subprocess.py:86} INFO - Output:
[2025-05-23T08:11:13.602+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2025-05-23T08:11:13.603+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-05-23T08:11:14.202+0000] {subprocess.py:93} INFO - 25/05/23 08:11:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-23T08:11:19.320+0000] {subprocess.py:93} INFO - Spark session created successfully!
[2025-05-23T08:11:19.321+0000] {subprocess.py:93} INFO - Traceback (most recent call last):
[2025-05-23T08:11:19.321+0000] {subprocess.py:93} INFO -   File "/opt/spark/work-dir/modelisation.py", line 226, in <module>
[2025-05-23T08:11:19.325+0000] {subprocess.py:93} INFO -     .csv(hdfs_path)
[2025-05-23T08:11:19.328+0000] {subprocess.py:93} INFO -      ^^^^^^^^^^^^^^
[2025-05-23T08:11:19.329+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/pyspark/sql/readwriter.py", line 740, in csv
[2025-05-23T08:11:19.330+0000] {subprocess.py:93} INFO -     return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))
[2025-05-23T08:11:19.330+0000] {subprocess.py:93} INFO -                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[2025-05-23T08:11:19.331+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/lib/py4j.zip/py4j/java_gateway.py", line 1322, in __call__
[2025-05-23T08:11:19.331+0000] {subprocess.py:93} INFO -   File "/opt/bitnami/spark/python/pyspark/errors/exceptions/captured.py", line 185, in deco
[2025-05-23T08:11:19.332+0000] {subprocess.py:93} INFO -     raise converted from None
[2025-05-23T08:11:19.339+0000] {subprocess.py:93} INFO - pyspark.errors.exceptions.captured.AnalysisException: [PATH_NOT_FOUND] Path does not exist: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv.
[2025-05-23T08:11:19.900+0000] {subprocess.py:97} INFO - Command exited with return code 1
[2025-05-23T08:11:19.913+0000] {taskinstance.py:1943} ERROR - Task failed with exception
Traceback (most recent call last):
  File "/home/airflow/.local/lib/python3.9/site-packages/airflow/operators/bash.py", line 210, in execute
    raise AirflowException(
airflow.exceptions.AirflowException: Bash command failed. The command returned a non-zero exit code 1.
[2025-05-23T08:11:19.919+0000] {taskinstance.py:1400} INFO - Marking task as FAILED. dag_id=fetch_transaction_data_and_upload_to_hdfs, task_id=run_spark_modelisation_command, execution_date=20250522T000000, start_date=20250523T081106, end_date=20250523T081119
[2025-05-23T08:11:19.939+0000] {standard_task_runner.py:104} ERROR - Failed to execute job 30 for task run_spark_modelisation_command (Bash command failed. The command returned a non-zero exit code 1.; 12100)
[2025-05-23T08:11:19.974+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 1
[2025-05-23T08:11:20.001+0000] {taskinstance.py:2784} INFO - 0 downstream tasks scheduled from follow-on schedule check
[2025-05-23T09:27:43.285+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=non-requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [queued]>
[2025-05-23T09:27:43.303+0000] {taskinstance.py:1159} INFO - Dependencies all met for dep_context=requeueable deps ti=<TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [queued]>
[2025-05-23T09:27:43.304+0000] {taskinstance.py:1361} INFO - Starting attempt 2 of 2
[2025-05-23T09:27:43.325+0000] {taskinstance.py:1382} INFO - Executing <Task(BashOperator): run_spark_modelisation_command> on 2025-05-22 00:00:00+00:00
[2025-05-23T09:27:43.332+0000] {standard_task_runner.py:57} INFO - Started process 586 to run task
[2025-05-23T09:27:43.337+0000] {standard_task_runner.py:84} INFO - Running: ['***', 'tasks', 'run', 'fetch_transaction_data_and_upload_to_hdfs', 'run_spark_modelisation_command', 'scheduled__2025-05-22T00:00:00+00:00', '--job-id', '19', '--raw', '--subdir', 'DAGS_FOLDER/ETL.py', '--cfg-path', '/tmp/tmp1j293qcp']
[2025-05-23T09:27:43.343+0000] {standard_task_runner.py:85} INFO - Job 19: Subtask run_spark_modelisation_command
[2025-05-23T09:27:43.431+0000] {task_command.py:415} INFO - Running <TaskInstance: fetch_transaction_data_and_upload_to_hdfs.run_spark_modelisation_command scheduled__2025-05-22T00:00:00+00:00 [running]> on host 6c702459c558
[2025-05-23T09:27:43.748+0000] {taskinstance.py:1660} INFO - Exporting env vars: AIRFLOW_CTX_DAG_OWNER='***' AIRFLOW_CTX_DAG_ID='fetch_transaction_data_and_upload_to_hdfs' AIRFLOW_CTX_TASK_ID='run_spark_modelisation_command' AIRFLOW_CTX_EXECUTION_DATE='2025-05-22T00:00:00+00:00' AIRFLOW_CTX_TRY_NUMBER='2' AIRFLOW_CTX_DAG_RUN_ID='scheduled__2025-05-22T00:00:00+00:00'
[2025-05-23T09:27:43.750+0000] {subprocess.py:63} INFO - Tmp dir root location: /tmp
[2025-05-23T09:27:43.752+0000] {subprocess.py:75} INFO - Running command: ['/bin/bash', '-c', 'docker exec spark-master python3 modelisation.py']
[2025-05-23T09:27:43.767+0000] {subprocess.py:86} INFO - Output:
[2025-05-23T09:27:47.879+0000] {subprocess.py:93} INFO - Setting default log level to "WARN".
[2025-05-23T09:27:47.880+0000] {subprocess.py:93} INFO - To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).
[2025-05-23T09:27:48.711+0000] {subprocess.py:93} INFO - 25/05/23 09:27:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
[2025-05-23T09:27:58.717+0000] {subprocess.py:93} INFO - [Stage 0:>                                                          (0 + 1) / 1]25/05/23 09:27:58 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:27:58.718+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:27:58.719+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:27:58.719+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:27:58.720+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:01.719+0000] {subprocess.py:93} INFO -                                                                                 25/05/23 09:28:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:01.720+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:01.721+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:01.722+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:01.722+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:01.723+0000] {subprocess.py:93} INFO - 25/05/23 09:28:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:01.723+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:01.724+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:01.725+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:01.726+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:01.925+0000] {subprocess.py:93} INFO - [Stage 3:>                                                          (0 + 1) / 1]25/05/23 09:28:01 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:01.926+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:01.927+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:01.927+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:01.928+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:03.628+0000] {subprocess.py:93} INFO - [Stage 3:>                  (0 + 1) / 1][Stage 4:>                  (0 + 1) / 1]                                                                                25/05/23 09:28:03 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:03.629+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:03.630+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:03.631+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:03.632+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:07.007+0000] {subprocess.py:93} INFO - [Stage 12:>                                                         (0 + 1) / 1]                                                                                [Stage 13:>                                                         (0 + 1) / 1]25/05/23 09:28:07 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:07.008+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:07.009+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:07.010+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:07.011+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:09.063+0000] {subprocess.py:93} INFO - [Stage 15:>                                                         (0 + 1) / 1]                                                                                25/05/23 09:28:09 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:09.064+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:09.065+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:09.066+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:09.067+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:10.279+0000] {subprocess.py:93} INFO - 25/05/23 09:28:10 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:10.281+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:10.290+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:10.294+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:10.295+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:11.311+0000] {subprocess.py:93} INFO - 25/05/23 09:28:11 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:11.312+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:11.313+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:11.314+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:11.315+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:12.371+0000] {subprocess.py:93} INFO - 25/05/23 09:28:12 WARN CSVHeaderChecker: CSV header does not conform to the schema.
[2025-05-23T09:28:12.372+0000] {subprocess.py:93} INFO -  Header: Transaction ID, Customer ID, Transaction Amount, Transaction Date, Payment Method, Product Category, Quantity, Customer Age, Customer Location, Device Used, IP Address, Shipping Address, Billing Address, Is Fraudulent, Account Age Days, Transaction Hour
[2025-05-23T09:28:12.373+0000] {subprocess.py:93} INFO -  Schema: Transaction_ID, Customer_ID, Transaction_Amount, Transaction_Date, Payment_Method, Product_Category, Quantity, Customer_Age, Customer_Location, Device_Used, IP_Address, Shipping_Address, Billing_Address, Is_Fraudulent, Account_Age_Days, Transaction_Hour
[2025-05-23T09:28:12.373+0000] {subprocess.py:93} INFO - Expected: Transaction_ID but found: Transaction ID
[2025-05-23T09:28:12.374+0000] {subprocess.py:93} INFO - CSV file: hdfs://namenode:9000/user/root/transactions/YYYY=2025/MM=05/DD=23/transaction_data.csv
[2025-05-23T09:28:14.740+0000] {subprocess.py:93} INFO - [Stage 25:>                                                         (0 + 1) / 1][Stage 27:>                                                         (0 + 1) / 1]                                                                                Spark session created successfully!
[2025-05-23T09:28:14.741+0000] {subprocess.py:93} INFO - Data loaded successfully. Row count: 23634
[2025-05-23T09:28:14.742+0000] {subprocess.py:93} INFO - Data saved to HDFS successfully!
[2025-05-23T09:28:15.246+0000] {subprocess.py:97} INFO - Command exited with return code 0
[2025-05-23T09:28:15.283+0000] {taskinstance.py:1400} INFO - Marking task as SUCCESS. dag_id=fetch_transaction_data_and_upload_to_hdfs, task_id=run_spark_modelisation_command, execution_date=20250522T000000, start_date=20250523T092743, end_date=20250523T092815
[2025-05-23T09:28:15.351+0000] {local_task_job_runner.py:228} INFO - Task exited with return code 0
[2025-05-23T09:28:15.385+0000] {taskinstance.py:2784} INFO - 1 downstream tasks scheduled from follow-on schedule check
